{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b54df4-8d9b-48ab-8e62-014588aaa5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1_raw_raspagem.py\n",
    "# Notebook para raspagem do Brasileir√£o usando API alternativa\n",
    "\n",
    "# ----------------------\n",
    "# Cria catalog e schema se n√£o existir\n",
    "# ----------------------\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS brasileirao\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS brasileirao.raw\")\n",
    "\n",
    "# ----------------------\n",
    "# IMPORTS\n",
    "# ----------------------\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Inicializa Spark\n",
    "spark = SparkSession.builder.appName(\"Brasileirao_Raw\").getOrCreate()\n",
    "\n",
    "# ----------------------\n",
    "# CONFIGURA√á√ÉO DELTA\n",
    "# ----------------------\n",
    "delta_table = \"brasileirao.raw.tb_partidas_raw\"\n",
    "\n",
    "# ----------------------\n",
    "# PRIMEIRO: VERIFICAR SE A TABELA J√Å EXISTE E QUAL SEU SCHEMA\n",
    "# ----------------------\n",
    "table_exists = spark.catalog.tableExists(delta_table)\n",
    "if table_exists:\n",
    "    print(f\"üìã Tabela {delta_table} j√° existe. Schema atual:\")\n",
    "    spark.sql(f\"DESCRIBE {delta_table}\").show(truncate=False)\n",
    "else:\n",
    "    print(f\"üìã Tabela {delta_table} ser√° criada\")\n",
    "\n",
    "# ----------------------\n",
    "# DADOS MOCK DE ALTA QUALIDADE (Brasileir√£o 2025)\n",
    "# ----------------------\n",
    "def create_mock_data():\n",
    "    \"\"\"Cria dados realistas do Brasileir√£o 2025\"\"\"\n",
    "    times_brasileirao = [\n",
    "        \"Flamengo\", \"Palmeiras\", \"Atl√©tico-MG\", \"Corinthians\", \"Fluminense\",\n",
    "        \"S√£o Paulo\", \"Gr√™mio\", \"Internacional\", \"Santos\", \"Botafogo\",\n",
    "        \"Athletico-PR\", \"Bragantino\", \"Cruzeiro\", \"Vasco\", \"Bahia\",\n",
    "        \"Fortaleza\", \"Cuiab√°\", \"Goi√°s\", \"Coritiba\", \"Am√©rica-MG\"\n",
    "    ]\n",
    "    \n",
    "    partidas = []\n",
    "    data_base = datetime(2025, 5, 1)\n",
    "    \n",
    "    # Simula as primeiras 5 rodadas\n",
    "    for rodada_num in range(1, 6):\n",
    "        # Em cada rodada, 10 jogos\n",
    "        for i in range(0, 20, 2):\n",
    "            mandante = times_brasileirao[i]\n",
    "            visitante = times_brasileirao[i + 1] if i + 1 < len(times_brasileirao) else times_brasileirao[0]\n",
    "            \n",
    "            # Gera placares realistas\n",
    "            placar_mandante = i % 3  # 0, 1, 2, 0, 1...\n",
    "            placar_visitante = (i + 1) % 2  # 1, 0, 1, 0...\n",
    "            \n",
    "            # Data da partida (avan√ßa 2 dias a cada jogo)\n",
    "            data_partida = (data_base + timedelta(days=(rodada_num-1)*7 + i//2)).strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            partidas.append(Row(\n",
    "                Mandante=mandante,\n",
    "                Visitante=visitante,\n",
    "                Placar=f\"{placar_mandante} x {placar_visitante}\",\n",
    "                Data=data_partida,\n",
    "                Estadio=\"Maracan√£\" if mandante == \"Flamengo\" else \"Allianz Parque\" if mandante == \"Palmeiras\" else \"Arena MRV\",\n",
    "                Rodada=str(rodada_num),\n",
    "                Status=\"Finalizado\",\n",
    "                Temporada=\"2025\"\n",
    "            ))\n",
    "    \n",
    "    return partidas\n",
    "\n",
    "print(\"üìã Usando dados mock do Brasileir√£o 2025...\")\n",
    "partidas = create_mock_data()\n",
    "\n",
    "# ----------------------\n",
    "# CRIAR SPARK DATAFRAME\n",
    "# ----------------------\n",
    "schema = StructType([\n",
    "    StructField(\"Mandante\", StringType(), True),\n",
    "    StructField(\"Visitante\", StringType(), True),\n",
    "    StructField(\"Placar\", StringType(), True),\n",
    "    StructField(\"Data\", StringType(), True),\n",
    "    StructField(\"Estadio\", StringType(), True),\n",
    "    StructField(\"Rodada\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Temporada\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(partidas, schema=schema)\n",
    "\n",
    "# ----------------------\n",
    "# SALVAR DELTA TABLE COM SCHEMA CORRIGIDO\n",
    "# ----------------------\n",
    "try:\n",
    "    # Primeiro tenta com mergeSchema (mais seguro)\n",
    "    df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(delta_table)\n",
    "    print(f\"‚úÖ Delta Table salva com mergeSchema: {delta_table}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MergeSchema falhou: {e}\")\n",
    "    print(\"üîÑ Tentando com overwriteSchema...\")\n",
    "    \n",
    "    # Se merge falhar, usa overwriteSchema\n",
    "    df.write.format(\"delta\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(delta_table)\n",
    "    print(f\"‚úÖ Delta Table salva com overwriteSchema: {delta_table}\")\n",
    "\n",
    "# ----------------------\n",
    "# MOSTRAR DADOS\n",
    "# ----------------------\n",
    "print(f\"\\nüìä Dados salvos ({df.count()} partidas):\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ----------------------\n",
    "# VERIFICAR O SCHEMA FINAL\n",
    "# ----------------------\n",
    "print(\"\\nüóÑÔ∏è  Schema final da tabela:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {delta_table}\").show(truncate=False)\n",
    "\n",
    "# ----------------------\n",
    "# LIMPAR TABELA EXISTENTE SE PRECISAR (OPCIONAL)\n",
    "# ----------------------\n",
    "# Se quiser come√ßar do zero, pode dropar a tabela primeiro:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {delta_table}\")\n",
    "# print(f\"üßπ Tabela {delta_table} dropada\")\n",
    "\n",
    "# ----------------------\n",
    "# ALTERNATIVA: CRIAR TABELA DO ZERO SE N√ÉO EXISTIR\n",
    "# ----------------------\n",
    "if not table_exists:\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table)\n",
    "    print(f\"‚úÖ Nova tabela criada: {delta_table}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline conclu√≠do com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scraping deep",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
