{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4143e7-2735-462c-908f-b6e3512de887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC ## üìã Notebook 01 - Parte 1: Configura√ß√£o Inicial\n",
    "# MAGIC \n",
    "# MAGIC Vamos testar passo a passo a configura√ß√£o do ambiente.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### **Passo 1: Verificar se podemos criar um Catalog**\n",
    "# MAGIC \n",
    "# MAGVamos tentar criar o catalog e ver o que acontece na free edition.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Tenta criar o catalog\n",
    "try:\n",
    "    spark.sql(\"CREATE CATALOG IF NOT EXISTS brasileirao2025\")\n",
    "    print(\"‚úÖ Catalog 'brasileirao2025' criado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao criar catalog: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Provavelmente estamos na free edition - usando hive_metastore\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### **Passo 2: Listar os catalogs dispon√≠veis**\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Mostra todos os catalogs dispon√≠veis\n",
    "try:\n",
    "    catalogs = spark.sql(\"SHOW CATALOGS\")\n",
    "    display(catalogs)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao listar catalogs: {e}\")\n",
    "    print(\"Usando hive_metastore como padr√£o\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### **Passo 3: Definir nossa estrat√©gia com base no resultado**\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Vamos ver qual catalog estamos usando\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "print(f\"üìä Catalog atual: {current_catalog}\")\n",
    "\n",
    "# Estrat√©gia: Se n√£o conseguimos criar catalog, usamos hive_metastore\n",
    "if current_catalog == \"hive_metastore\":\n",
    "    schema_name = \"brasileirao2025_bronze\"\n",
    "    full_table_path = f\"hive_metastore.{schema_name}.jogos_raw\"\n",
    "    print(f\"üéØ Usando hive_metastore com schema: {schema_name}\")\n",
    "else:\n",
    "    schema_name = \"bronze\"\n",
    "    full_table_path = f\"brasileirao2025.{schema_name}.jogos_raw\"\n",
    "    print(f\"üéØ Usando Unity Catalog: {full_table_path}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### **Passo 4: Criar o schema/database**\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Criar o schema onde vamos guardar nossos dados\n",
    "try:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n",
    "    print(f\"‚úÖ Schema '{schema_name}' criado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao criar schema: {e}\")\n",
    "\n",
    "# Mostrar todos os schemas/databases\n",
    "print(\"\\nüìã Schemas dispon√≠veis:\")\n",
    "databases = spark.sql(\"SHOW DATABASES\")\n",
    "display(databases)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_Ingestao_Dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
